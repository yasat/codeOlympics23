{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oF6IBgFDTAQh",
    "outputId": "7711090e-aaf4-49e7-9cab-87e5ce97ba25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                words\n",
      "2                                                    \n",
      "4            rom fairest creatures we desire increase\n",
      "5           hat thereby beauty’s rose might never die\n",
      "6              ut as the riper should by time decease\n",
      "7                is tender heir might bear his memory\n",
      "8         ut thou contracted to thine own bright eyes\n",
      "9   eed’st thy light’s flame with selfsubstantial ...\n",
      "10                aking a famine where abundance lies\n",
      "11         hyself thy foe to thy sweet self too cruel\n",
      "12        hou that art now the world’s fresh ornament\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "response = requests.get('https://www.gutenberg.org/cache/epub/100/pg100.txt')\n",
    "\n",
    "text = response.text\n",
    "\n",
    "para = list()\n",
    "\n",
    "for i in text.split(\"THE SONNETS\")[2].split(\"\\r\\n\"):\n",
    "  para.append(i)\n",
    "\n",
    "shakes = pd.DataFrame(para)\n",
    "\n",
    "shakes.head(100)\n",
    "\n",
    "shakes = shakes.rename(columns={0:\"words\"})\n",
    "\n",
    "shakes['words'] = shakes.words.apply(lambda x: x.strip())\n",
    "\n",
    "shakes = shakes[shakes['words'] != \"\"]\n",
    "\n",
    "shakes['words'] = shakes.words.apply(lambda x: re.sub(\"[0-9]\\r\\n\",\"\", x))\n",
    "shakes['words'] = shakes.words.apply(lambda x: re.sub(\"[0-9]\",\"\", x))\n",
    "shakes['words'] = shakes.words.apply(lambda x: re.sub(\"[,-\\[\\]'$({})]\",\"\",x))\n",
    "shakes['words'] = shakes.words.apply(lambda x: x.replace(\"\\r\", \"\"))\n",
    "shakes['words'] = shakes.words.apply(lambda x: x.replace(\"\\n\", \"\"))\n",
    "\n",
    "lines = [line for line in list(shakes.words) if line != \"\"]\n",
    "\n",
    "\n",
    "print(shakes.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y07sQFHXd_T6",
    "outputId": "fee81928-1f75-493b-d966-00a0da6ebb26"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 124309/124309 [10:23<00:00, 199.33it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "124309"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tqdm\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# nlp.max_length = 9999999\n",
    "\n",
    "def text_pipeline_spacy(lines):\n",
    "    p_tokens = []\n",
    "    \n",
    "    #beware!!!! this ate my laptop resources... rerun this at risk!!!!! (I personally wouldn't risk it!!!) ~Team18\n",
    "    doc = nlp(lines)\n",
    "    for t in doc:\n",
    "        if not t.is_stop and not t.is_punct and not t.is_space:\n",
    "            p_tokens.append(t.lemma_.lower())\n",
    "    return p_tokens\n",
    "\n",
    "lines = [text_pipeline_spacy(line) for line in tqdm.tqdm(lines)]\n",
    "\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !conda install gensim\n",
    "# import gensim\n",
    "# from gensim.utils import  simple_preprocess\n",
    "\n",
    "# ngram2 = gensim.models.Phrases(lines, min_count=1, threshold=1)\n",
    "# ngram3 = gensim.models.Phrases(ngram2[lines], min_count=2, threshold=1)\n",
    "# ngram4 = gensim.models.Phrases(ngram3[ngram2[lines]], min_count=1, threshold=1)\n",
    "# ngram5 = gensim.models.Phrases(ngram4[ngram3[ngram2[lines]]], min_count=2, threshold=1)\n",
    "# ngram2_phraser = gensim.models.phrases.Phraser(ngram2)\n",
    "# ngram3_phraser = gensim.models.phrases.Phraser(ngram3)\n",
    "# ngram4_phraser = gensim.models.phrases.Phraser(ngram4)\n",
    "# ngram5_phraser = gensim.models.phrases.Phraser(ngram5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def bigramsfn(lines):\n",
    "#     lines = [ngram2_phraser[line] for line in lines]\n",
    "#     return lines\n",
    "\n",
    "# ngrams2_words = bigramsfn(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def trigramsfn(lines):\n",
    "#     lines = [ngram3_phraser[ngram2_phraser[line]] for line in lines]\n",
    "#     return lines\n",
    "\n",
    "# ngrams3_words = trigramsfn(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at ngrams = 2\n",
      "[(('thou', 'art'), 432), (('y', 'lord'), 344), (('thou', 'hast'), 276), (('o', 'th'), 257), (('thou', 'shalt'), 182), (('good', 'lord'), 182), (('f', 'thou'), 180), (('art', 'thou'), 175), (('hou', 'art'), 161), (('ir', 'ohn'), 156)]\n",
      "----------\n",
      "at ngrams = 3\n",
      "[(('y', 'good', 'lord'), 38), (('hy', 'dost', 'thou'), 34), (('hat', 'art', 'thou'), 30), (('ve', 'thy', 'hand'), 27), (('hat', 'wouldst', 'thou'), 25), (('o', 'good', 'lord'), 24), (('hat', 'dost', 'thou'), 24), (('ir', 'ohn', 'alstaff'), 24), (('thee', 'thou', 'art'), 23), (('nd', 'thou', 'shalt'), 22)]\n",
      "----------\n",
      "at ngrams = 4\n",
      "[(('roject', 'utenberg', 'electronic', 'work'), 18), (('roject', 'utenberg', 'iterary', 'rchive'), 13), (('utenberg', 'iterary', 'rchive', 'oundation'), 13), (('elmont', 'room', 'ortia', 'house'), 12), (('ossillon', 'room', 'ountess', 'palace'), 10), (('cene', 'lexandria', 'oom', 'alace'), 8), (('lexandria', 'oom', 'alace', 'cene'), 8), (('lexandria', 'oom', 'alace', 'nter'), 8), (('xeunt', 'ondon', 'palace', 'nter'), 8), (('recian', 'camp', 'efore', 'tent'), 8)]\n",
      "----------\n",
      "at ngrams = 5\n",
      "[(('roject', 'utenberg', 'iterary', 'rchive', 'oundation'), 13), (('cene', 'lexandria', 'oom', 'alace', 'cene'), 8), (('xeunt', 'lexandria', 'oom', 'alace', 'nter'), 7), (('friend', 'ntony', 'friend', 'ntony', 'friend'), 6), (('room', 'astle', 'cene', 'nother', 'room'), 6), (('astle', 'cene', 'nother', 'room', 'astle'), 6), (('cene', 'nother', 'room', 'astle', 'cene'), 6), (('cene', 'elmont', 'room', 'ortia', 'house'), 6), (('elmont', 'room', 'ortia', 'house', 'cene'), 6), (('xeunt', 'elmont', 'room', 'ortia', 'house'), 6)]\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "stopwords = [\"\", \"™\"]\n",
    "\n",
    "words = [word for line in lines for word in line if word not in stopwords]\n",
    "\n",
    "for i in range(2,6):\n",
    "    ngram_counts = Counter(ngrams(words, i))\n",
    "    print(\"at ngrams =\",i)\n",
    "    print(ngram_counts.most_common(10))\n",
    "    print(\"----------\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
